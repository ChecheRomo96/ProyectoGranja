{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdb3bf2e",
   "metadata": {},
   "source": [
    "# Avance 2 — Ingeniería de Características (FE) + Selección/Extracción (CRISP-ML: Data Preparation)\n",
    "\n",
    "**Objetivo:** predecir **Producción (kg)** y habilitar **optimización** del proceso.\n",
    "\n",
    "**Entrega:** renombra este archivo como `Avance2.#Equipo.ipynb` antes de subirlo al repositorio del equipo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1831c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# 0) Imports + Configuración\n",
    "# ============================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "\n",
    "# Seaborn / statsmodels (opcionales)\n",
    "try:\n",
    "    import seaborn as sns\n",
    "    HAVE_SNS = True\n",
    "except Exception:\n",
    "    HAVE_SNS = False\n",
    "\n",
    "try:\n",
    "    import statsmodels.api as sm\n",
    "    HAVE_STATSMODELS = True\n",
    "except Exception:\n",
    "    HAVE_STATSMODELS = False\n",
    "\n",
    "print(\"HAVE_SNS:\", HAVE_SNS, \"| HAVE_STATSMODELS:\", HAVE_STATSMODELS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c914a16",
   "metadata": {},
   "source": [
    "## 1) Carga de datos (raw) y parsing de fechas\n",
    "\n",
    "**Justificación:** convertir datos crudos del mundo real a un formato consistente y usable en ML; en particular, estandarizar timestamps para integrar clima por hora.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e47158f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rutas sandbox (este entorno) / rutas típicas de repo (data/)\n",
    "SANDBOX_UNIFICADO = Path(\"/mnt/data/Dataset_Unificado.csv\")\n",
    "SANDBOX_CLIMA     = Path(\"/mnt/data/clima_horario_openmeteo_2025 (1).csv\")\n",
    "\n",
    "REPO_UNIFICADO = Path(\"../data/Dataset_Unificado.csv\")\n",
    "REPO_CLIMA     = Path(\"../data/clima_horario_openmeteo_2025.csv\")\n",
    "\n",
    "def read_csv_smart(path: Path) -> pd.DataFrame:\n",
    "    try:\n",
    "        return pd.read_csv(path, encoding=\"utf-8\")\n",
    "    except UnicodeDecodeError:\n",
    "        return pd.read_csv(path, encoding=\"ISO-8859-1\")\n",
    "\n",
    "if SANDBOX_UNIFICADO.exists() and SANDBOX_CLIMA.exists():\n",
    "    df_raw = read_csv_smart(SANDBOX_UNIFICADO)\n",
    "    clima_raw = read_csv_smart(SANDBOX_CLIMA)\n",
    "    print(\"Leyendo desde /mnt/data (sandbox)\")\n",
    "else:\n",
    "    df_raw = read_csv_smart(REPO_UNIFICADO)\n",
    "    clima_raw = read_csv_smart(REPO_CLIMA)\n",
    "    print(\"Leyendo desde ../data (repo)\")\n",
    "\n",
    "print(\"Dataset_Unificado:\", df_raw.shape)\n",
    "print(\"Clima:\", clima_raw.shape)\n",
    "\n",
    "df = df_raw.copy()\n",
    "clima = clima_raw.copy()\n",
    "\n",
    "df[\"ts_inicio\"] = pd.to_datetime(df[\"Hora de inicio\"], dayfirst=True, errors=\"coerce\")\n",
    "clima[\"ts_clima\"] = pd.to_datetime(clima[\"time\"], errors=\"coerce\")\n",
    "\n",
    "df = df.dropna(subset=[\"ts_inicio\"]).sort_values(\"ts_inicio\").reset_index(drop=True)\n",
    "clima = clima.dropna(subset=[\"ts_clima\"]).sort_values(\"ts_clima\").reset_index(drop=True)\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f49bbae",
   "metadata": {},
   "source": [
    "## 2) Conversión de duraciones (texto → numérico)\n",
    "\n",
    "**Justificación:** duraciones tipo `mm:ss` / `hh:mm` se convierten a segundos/minutos para permitir escalamiento, transformaciones y modelos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0155bb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mmss_to_seconds(x):\n",
    "    if pd.isna(x): return np.nan\n",
    "    s = str(x).strip()\n",
    "    if \":\" not in s: return np.nan\n",
    "    parts = s.split(\":\")\n",
    "    if len(parts) != 2: return np.nan\n",
    "    try:\n",
    "        mm, ss = parts\n",
    "        return int(mm) * 60 + int(ss)\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "\n",
    "def hhmm_to_minutes(x):\n",
    "    if pd.isna(x): return np.nan\n",
    "    s = str(x).strip()\n",
    "    if \":\" not in s: return np.nan\n",
    "    parts = s.split(\":\")\n",
    "    if len(parts) != 2: return np.nan\n",
    "    try:\n",
    "        hh, mm = parts\n",
    "        return int(hh) * 60 + int(mm)\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "\n",
    "if \"Duración (mm:ss)\" in df.columns:\n",
    "    df[\"duracion_s\"] = df[\"Duración (mm:ss)\"].apply(mmss_to_seconds)\n",
    "else:\n",
    "    df[\"duracion_s\"] = np.nan\n",
    "\n",
    "if \"Intervalo de ordeño (hh:mm)\" in df.columns:\n",
    "    df[\"intervalo_ordeño_min\"] = df[\"Intervalo de ordeño (hh:mm)\"].apply(hhmm_to_minutes)\n",
    "else:\n",
    "    df[\"intervalo_ordeño_min\"] = np.nan\n",
    "\n",
    "# Coerción a numérico de columnas típicas (si existen)\n",
    "maybe_numeric = [\n",
    "    \"Producción (kg)\",\n",
    "    \"Duracion de Incremento\",\n",
    "    \"Duracion de Decremento\",\n",
    "    \"Tiempo de Incremento\",\n",
    "    \"Tiempo de Decremento\",\n",
    "    \"Ubre\",\n",
    "    \"Pezón\",\n",
    "    \"Número de ordeño\",\n",
    "    \"Num Lactacion\",\n",
    "]\n",
    "for c in maybe_numeric:\n",
    "    if c in df.columns:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "df[[c for c in [\"Duración (mm:ss)\",\"duracion_s\",\"Intervalo de ordeño (hh:mm)\",\"intervalo_ordeño_min\",\"Producción (kg)\"] if c in df.columns]].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c77f1e",
   "metadata": {},
   "source": [
    "## 3) Integración con clima (enrichment)\n",
    "\n",
    "**Justificación:** variables ambientales pueden afectar el proceso. Como el clima está por hora, se usa `merge_asof` con tolerancia ±2h.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f598cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "clima_feats = clima.rename(columns={\n",
    "    \"temperature_2m\": \"temp_C\",\n",
    "    \"relative_humidity_2m\": \"hum_rel\",\n",
    "    \"pressure_msl\": \"pres_msl\",\n",
    "    \"precipitation\": \"precip_mm\",\n",
    "    \"wind_speed_10m\": \"wind_ms\",\n",
    "}).copy()\n",
    "\n",
    "clima_keep = [\"ts_clima\"]\n",
    "for c in [\"temp_C\",\"hum_rel\",\"pres_msl\",\"precip_mm\",\"wind_ms\"]:\n",
    "    if c in clima_feats.columns:\n",
    "        clima_keep.append(c)\n",
    "clima_feats = clima_feats[clima_keep].sort_values(\"ts_clima\").reset_index(drop=True)\n",
    "\n",
    "df = pd.merge_asof(\n",
    "    df.sort_values(\"ts_inicio\"),\n",
    "    clima_feats,\n",
    "    left_on=\"ts_inicio\",\n",
    "    right_on=\"ts_clima\",\n",
    "    direction=\"nearest\",\n",
    "    tolerance=pd.Timedelta(\"2H\"),\n",
    ")\n",
    "\n",
    "df[[\"ts_inicio\"] + [c for c in [\"temp_C\",\"hum_rel\",\"pres_msl\",\"precip_mm\",\"wind_ms\"] if c in df.columns]].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0137d106",
   "metadata": {},
   "source": [
    "## 4) 2.3 — Generación de nuevas características\n",
    "\n",
    "### 4.1 Temporales y cíclicas\n",
    "**Justificación:** la periodicidad diaria/anual se representa con seno/coseno para evitar discontinuidades (23→0).\n",
    "\n",
    "> Nota: se crea `is_weekend`, pero si resulta insignificante (ANOVA/correlación), se excluye del entrenamiento para reducir ruido.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8846e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = df[\"ts_inicio\"]\n",
    "df[\"hour\"] = ts.dt.hour\n",
    "df[\"dow\"] = ts.dt.dayofweek\n",
    "df[\"month\"] = ts.dt.month\n",
    "df[\"dayofyear\"] = ts.dt.dayofyear\n",
    "df[\"is_weekend\"] = (df[\"dow\"] >= 5).astype(int)\n",
    "\n",
    "df[\"hour_sin\"] = np.sin(2*np.pi*df[\"hour\"]/24.0)\n",
    "df[\"hour_cos\"] = np.cos(2*np.pi*df[\"hour\"]/24.0)\n",
    "df[\"doy_sin\"]  = np.sin(2*np.pi*df[\"dayofyear\"]/366.0)\n",
    "df[\"doy_cos\"]  = np.cos(2*np.pi*df[\"dayofyear\"]/366.0)\n",
    "\n",
    "df[[\"ts_inicio\",\"hour\",\"dow\",\"month\",\"dayofyear\",\"is_weekend\",\"hour_sin\",\"hour_cos\"]].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0f48e2",
   "metadata": {},
   "source": [
    "### 4.2 Eficiencia y dinámica del proceso\n",
    "**Justificación:** para optimización interesa eficiencia (`kg/min`) y proporciones normalizadas (`inc_ratio`, `dec_ratio`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309e1821",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"duracion_min\"] = df[\"duracion_s\"] / 60.0\n",
    "\n",
    "if \"Producción (kg)\" in df.columns:\n",
    "    df[\"kg_por_min\"] = np.where(df[\"duracion_min\"] > 0, df[\"Producción (kg)\"] / df[\"duracion_min\"], np.nan)\n",
    "else:\n",
    "    df[\"kg_por_min\"] = np.nan\n",
    "\n",
    "if \"Tiempo de Incremento\" in df.columns:\n",
    "    df[\"inc_ratio\"] = np.where(df[\"duracion_s\"] > 0, df[\"Tiempo de Incremento\"] / df[\"duracion_s\"], np.nan)\n",
    "if \"Tiempo de Decremento\" in df.columns:\n",
    "    df[\"dec_ratio\"] = np.where(df[\"duracion_s\"] > 0, df[\"Tiempo de Decremento\"] / df[\"duracion_s\"], np.nan)\n",
    "\n",
    "df[[c for c in [\"Producción (kg)\",\"duracion_min\",\"kg_por_min\",\"inc_ratio\",\"dec_ratio\"] if c in df.columns]].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3fd889",
   "metadata": {},
   "source": [
    "## 5) Discretización / binning (2.3)\n",
    "\n",
    "**Justificación:** capturar no-linealidad y producir insights operativos por rangos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644b5fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"duracion_min\" in df.columns:\n",
    "    df[\"duracion_bin_q\"] = pd.qcut(df[\"duracion_min\"], q=5, duplicates=\"drop\")\n",
    "\n",
    "if \"temp_C\" in df.columns:\n",
    "    df[\"temp_bin\"] = pd.cut(df[\"temp_C\"], bins=[-10, 0, 10, 20, 30, 40, 60])\n",
    "\n",
    "df[[c for c in [\"duracion_min\",\"duracion_bin_q\",\"temp_C\",\"temp_bin\"] if c in df.columns]].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9263cd56",
   "metadata": {},
   "source": [
    "## 6) Codificación (2.3)\n",
    "\n",
    "**Justificación:** ML requiere variables numéricas.  \n",
    "- One-hot: baja cardinalidad  \n",
    "- Frequency encoding: alta cardinalidad (evita explosión dimensional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb6c890",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = [\n",
    "    \"Destino Leche\",\n",
    "    \"Tipo de evento\",\n",
    "    \"MS\",\n",
    "    \"Usuario\",\n",
    "    \"Usuario.1\",\n",
    "    \"Programa de lavado\",\n",
    "    \"Razón de la desviación\",\n",
    "    \"duracion_bin_q\",\n",
    "    \"temp_bin\",\n",
    "]\n",
    "cat_cols = [c for c in cat_cols if c in df.columns]\n",
    "\n",
    "for c in cat_cols:\n",
    "    df[c] = df[c].astype(str).str.strip()\n",
    "    df.loc[df[c].isin([\"nan\",\"NaN\",\"None\",\"\"]), c] = np.nan\n",
    "\n",
    "def frequency_encode(series: pd.Series) -> pd.Series:\n",
    "    freq = series.value_counts(dropna=True)\n",
    "    return series.map(freq).fillna(0).astype(float)\n",
    "\n",
    "high_card, low_card = [], []\n",
    "for c in cat_cols:\n",
    "    nuniq = df[c].nunique(dropna=True)\n",
    "    (high_card if nuniq > 30 else low_card).append(c)\n",
    "\n",
    "for c in high_card:\n",
    "    df[c + \"_freq\"] = frequency_encode(df[c])\n",
    "\n",
    "df_ohe = pd.get_dummies(df[low_card], prefix=low_card, dummy_na=True) if low_card else pd.DataFrame(index=df.index)\n",
    "\n",
    "print(\"One-hot:\", low_card)\n",
    "print(\"Freq-encoding:\", high_card)\n",
    "df_ohe.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9be25ad",
   "metadata": {},
   "source": [
    "## 7) Matriz de features X y target y\n",
    "\n",
    "Target: `Producción (kg)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda0002f",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = \"Producción (kg)\"\n",
    "assert target_col in df.columns, \"No se encontró la columna target 'Producción (kg)'.\"\n",
    "\n",
    "# Base numéricas + contexto + clima + ingeniería\n",
    "base_feature_cols = [\n",
    "    # Proceso\n",
    "    \"duracion_s\",\"duracion_min\",\"intervalo_ordeño_min\",\n",
    "    \"Duracion de Incremento\",\"Duracion de Decremento\",\n",
    "    \"Tiempo de Incremento\",\"Tiempo de Decremento\",\n",
    "    \"Ubre\",\"Pezón\",\"Número de ordeño\",\"Num Lactacion\",\n",
    "    # Tiempo (preferir cíclicas; is_weekend se excluye del set final por insignificancia frecuente)\n",
    "    \"hour\",\"dow\",\"month\",\"dayofyear\",\n",
    "    \"hour_sin\",\"hour_cos\",\"doy_sin\",\"doy_cos\",\n",
    "    # Clima\n",
    "    \"temp_C\",\"hum_rel\",\"pres_msl\",\"precip_mm\",\"wind_ms\",\n",
    "    # Diagnóstico\n",
    "    \"kg_por_min\",\"inc_ratio\",\"dec_ratio\",\n",
    "]\n",
    "base_feature_cols = [c for c in base_feature_cols if c in df.columns]\n",
    "\n",
    "freq_cols = [c for c in df.columns if c.endswith(\"_freq\")]\n",
    "\n",
    "X_num = df[base_feature_cols + freq_cols].copy()\n",
    "X_cat = df_ohe.copy()\n",
    "\n",
    "X = pd.concat([X_num, X_cat], axis=1)\n",
    "y = df[target_col].copy()\n",
    "\n",
    "mask = y.notna()\n",
    "X = X.loc[mask].reset_index(drop=True)\n",
    "y = y.loc[mask].reset_index(drop=True)\n",
    "\n",
    "print(\"X:\", X.shape, \"| y:\", y.shape)\n",
    "X.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f87f048",
   "metadata": {},
   "source": [
    "## 7.1 Variables controlables vs no controlables (optimización)\n",
    "\n",
    "**Justificación:** para optimizar, interesa entrenar/interpretar con variables que son *palancas* operativas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b08b6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "controlables = [\n",
    "    \"duracion_min\",\n",
    "    \"duracion_s\",\n",
    "    \"intervalo_ordeño_min\",\n",
    "    \"Tiempo de Incremento\",\n",
    "    \"Tiempo de Decremento\",\n",
    "    \"Duracion de Incremento\",\n",
    "    \"Duracion de Decremento\",\n",
    "]\n",
    "controlables = [c for c in controlables if c in X.columns]\n",
    "print(\"Controlables disponibles:\", controlables)\n",
    "\n",
    "X_control = X[controlables].copy()\n",
    "print(\"X_control:\", X_control.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02948645",
   "metadata": {},
   "source": [
    "## 8) 2.4 — Mitigar sesgos / acelerar convergencia (imputación + escalamiento + transformaciones)\n",
    "\n",
    "**Justificación:** muchos algoritmos no toleran NaNs y son sensibles a escala.  \n",
    "Se implementa un bloque **robusto** para asegurar que las matrices entren a sklearn sin NaN y sin columnas vacías/constantes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d872120e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, PowerTransformer\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED)\n",
    "\n",
    "def MakeSklearnSafe(Xtr: pd.DataFrame, Xte: pd.DataFrame):\n",
    "    Xtr2 = Xtr.copy()\n",
    "    Xte2 = Xte.copy()\n",
    "\n",
    "    # 1) Eliminar columnas 100% NaN en train\n",
    "    all_nan_cols = Xtr2.columns[Xtr2.isna().all()].tolist()\n",
    "    if len(all_nan_cols) > 0:\n",
    "        Xtr2 = Xtr2.drop(columns=all_nan_cols)\n",
    "        Xte2 = Xte2.drop(columns=all_nan_cols)\n",
    "\n",
    "    # 2) Imputar numéricas con mediana (mediana NaN -> 0), demás con 0\n",
    "    num_cols = Xtr2.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    other_cols = [c for c in Xtr2.columns if c not in num_cols]\n",
    "\n",
    "    med = Xtr2[num_cols].median(numeric_only=True).fillna(0)\n",
    "    Xtr2[num_cols] = Xtr2[num_cols].fillna(med)\n",
    "    Xte2[num_cols] = Xte2[num_cols].fillna(med)\n",
    "\n",
    "    Xtr2[other_cols] = Xtr2[other_cols].fillna(0)\n",
    "    Xte2[other_cols] = Xte2[other_cols].fillna(0)\n",
    "\n",
    "    # 3) Quitar columnas constantes (no aportan para lineales/PCA/FA)\n",
    "    stds = Xtr2[num_cols].std(numeric_only=True)\n",
    "    const_cols = stds[stds < 1e-12].index.tolist()\n",
    "    if len(const_cols) > 0:\n",
    "        Xtr2 = Xtr2.drop(columns=const_cols)\n",
    "        Xte2 = Xte2.drop(columns=const_cols)\n",
    "        num_cols = [c for c in num_cols if c not in const_cols]\n",
    "\n",
    "    # 4) Seguridad final\n",
    "    Xtr2 = Xtr2.fillna(0)\n",
    "    Xte2 = Xte2.fillna(0)\n",
    "\n",
    "    return Xtr2, Xte2, num_cols\n",
    "\n",
    "X_train_safe, X_test_safe, num_features_safe = MakeSklearnSafe(X_train, X_test)\n",
    "\n",
    "# StandardScaler\n",
    "scaler_std = StandardScaler()\n",
    "X_train_std = X_train_safe.copy()\n",
    "X_test_std  = X_test_safe.copy()\n",
    "X_train_std[num_features_safe] = scaler_std.fit_transform(X_train_safe[num_features_safe])\n",
    "X_test_std[num_features_safe]  = scaler_std.transform(X_test_safe[num_features_safe])\n",
    "\n",
    "# MinMax (para chi2)\n",
    "scaler_mm = MinMaxScaler()\n",
    "X_train_mm = pd.DataFrame(scaler_mm.fit_transform(X_train_safe), columns=X_train_safe.columns, index=X_train_safe.index)\n",
    "X_test_mm  = pd.DataFrame(scaler_mm.transform(X_test_safe), columns=X_test_safe.columns, index=X_test_safe.index)\n",
    "\n",
    "# Yeo–Johnson (opcional; soporta ceros/negativos)\n",
    "pt = PowerTransformer(method=\"yeo-johnson\", standardize=True)\n",
    "X_train_yj = X_train_safe.copy()\n",
    "X_test_yj  = X_test_safe.copy()\n",
    "if len(num_features_safe) > 0:\n",
    "    X_train_yj[num_features_safe] = pt.fit_transform(X_train_safe[num_features_safe])\n",
    "    X_test_yj[num_features_safe]  = pt.transform(X_test_safe[num_features_safe])\n",
    "\n",
    "# Matrices controlables escaladas\n",
    "controlables_std = [c for c in controlables if c in X_train_std.columns]\n",
    "X_control_train_std = X_train_std[controlables_std].copy()\n",
    "X_control_test_std  = X_test_std[controlables_std].copy()\n",
    "\n",
    "print(\"X_train_std:\", X_train_std.shape, \"| X_train_mm:\", X_train_mm.shape, \"| X_train_yj:\", X_train_yj.shape)\n",
    "print(\"X_control_train_std:\", X_control_train_std.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5ab2c5",
   "metadata": {},
   "source": [
    "## 9) Scatter plots (variables vs Producción) + curva por bins\n",
    "\n",
    "**Justificación:** detectar no-linealidad, saturación y outliers; la curva por bins es interpretable y estable (sin dependencias extra).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1640b1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables a graficar (ajusta a tu criterio)\n",
    "vars_scatter = [\n",
    "    \"duracion_min\",\n",
    "    \"intervalo_ordeño_min\",\n",
    "    \"Tiempo de Incremento\",\n",
    "    \"Tiempo de Decremento\",\n",
    "    \"Duracion de Incremento\",\n",
    "    \"Duracion de Decremento\",\n",
    "    \"Num Lactacion\",\n",
    "    \"Número de ordeño\",\n",
    "    \"temp_C\",\n",
    "    \"hum_rel\",\n",
    "    \"precip_mm\",\n",
    "    \"wind_ms\",\n",
    "    \"kg_por_min\",\n",
    "    # 'is_weekend' suele ser insignificante; si quieres confirmarlo visualmente, descomenta:\n",
    "    # \"is_weekend\",\n",
    "]\n",
    "vars_scatter = [v for v in vars_scatter if v in df.columns]\n",
    "\n",
    "plot_df = df.loc[mask, vars_scatter + [target_col]].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "def binned_curve(x, y, bins=20):\n",
    "    \"\"\"Curva de respuesta por bins (media por bin) para interpretación operativa.\"\"\"\n",
    "    tmp = pd.DataFrame({\"x\": x, \"y\": y}).dropna()\n",
    "    if len(tmp) < 50:\n",
    "        return None\n",
    "    tmp[\"bin\"] = pd.cut(tmp[\"x\"], bins=bins)\n",
    "    g = tmp.groupby(\"bin\", observed=True)[\"y\"].mean()\n",
    "    xmid = [iv.mid for iv in g.index]\n",
    "    return np.array(xmid), g.values\n",
    "\n",
    "for v in vars_scatter:\n",
    "    x = plot_df[v]\n",
    "    yv = plot_df[target_col]\n",
    "    m = x.notna() & yv.notna()\n",
    "    if m.sum() < 50:\n",
    "        continue\n",
    "\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.scatter(x[m], yv[m], alpha=0.25, s=12)\n",
    "    out = binned_curve(x[m], yv[m], bins=25)\n",
    "    if out is not None:\n",
    "        xm, ym = out\n",
    "        plt.plot(xm, ym, color=\"red\", linewidth=2)  # tendencia en rojo\n",
    "    plt.title(f\"Producción vs {v} (scatter + bins)\")\n",
    "    plt.xlabel(v)\n",
    "    plt.ylabel(\"Producción (kg)\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa4a2cb",
   "metadata": {},
   "source": [
    "## 10) Selección de características (filtrado)\n",
    "\n",
    "Incluye:\n",
    "- Umbral de varianza\n",
    "- Correlación (exploratoria)\n",
    "- Chi-cuadrado (requiere clases -> discretizamos y)\n",
    "- ANOVA (F-test) para regresión\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdeda334",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold, chi2, f_regression\n",
    "\n",
    "# 10.1 Umbral de varianza (sobre X_train_std)\n",
    "vt = VarianceThreshold(threshold=0.01)\n",
    "Xtr_vt = vt.fit_transform(X_train_std)\n",
    "kept_cols_vt = X_train_std.columns[vt.get_support()].tolist()\n",
    "print(\"VarThreshold: antes =\", X_train_std.shape[1], \"| después =\", len(kept_cols_vt))\n",
    "\n",
    "# 10.2 Correlación (sobre train, numéricas)\n",
    "train_corr = pd.concat([X_train_safe.select_dtypes(include=[np.number]), y_train.reset_index(drop=True)], axis=1).corr(numeric_only=True)[target_col]\n",
    "train_corr = train_corr.drop(labels=[target_col], errors=\"ignore\").sort_values(ascending=False)\n",
    "print(\"Top corr +:\", train_corr.head(10).to_dict())\n",
    "print(\"Top corr -:\", train_corr.tail(10).to_dict())\n",
    "\n",
    "# 10.3 Chi2 (para dependencia feature->clase). En regresión discretizamos y.\n",
    "y_train_bins = pd.qcut(y_train, q=3, labels=[\"baja\",\"media\",\"alta\"])\n",
    "chi2_vals, p_vals = chi2(X_train_mm, y_train_bins)\n",
    "chi2_df = pd.DataFrame({\"feature\": X_train_mm.columns, \"chi2\": chi2_vals, \"p_value\": p_vals}).sort_values(\"chi2\", ascending=False)\n",
    "chi2_df.head(15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f14475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.4 ANOVA / F-test para regresión (requiere X sin NaN)\n",
    "F, p = f_regression(X_train_std, y_train)\n",
    "anova_df = pd.DataFrame({\"feature\": X_train_std.columns, \"F\": F, \"p_value\": p}).sort_values(\"F\", ascending=False)\n",
    "anova_df.head(15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d60803",
   "metadata": {},
   "source": [
    "## 11) Extracción de características: PCA y Factor Analysis (FA)\n",
    "\n",
    "**Justificación:** reducir dimensionalidad, multicolinealidad y costo computacional manteniendo información (PCA) o factores latentes (FA).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b577cf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA, FactorAnalysis\n",
    "\n",
    "# PCA (95% varianza) sobre X_train_std\n",
    "pca = PCA(n_components=0.95, random_state=RANDOM_SEED)\n",
    "Xtr_pca = pca.fit_transform(X_train_std)\n",
    "Xte_pca = pca.transform(X_test_std)\n",
    "print(\"Componentes PCA:\", Xtr_pca.shape[1], \"| Varianza acum:\", float(pca.explained_variance_ratio_.sum()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628fc379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Factor Analysis: elegir un número razonable de factores (heurística)\n",
    "n_factors = min(10, X_train_std.shape[1])\n",
    "fa = FactorAnalysis(n_components=n_factors, random_state=RANDOM_SEED)\n",
    "Xtr_fa = fa.fit_transform(X_train_std)\n",
    "Xte_fa = fa.transform(X_test_std)\n",
    "print(\"Factores FA:\", Xtr_fa.shape[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d315ee",
   "metadata": {},
   "source": [
    "## 12) Baseline de modelos (validación rápida)\n",
    "\n",
    "**Justificación:** aunque el foco es Data Preparation, un baseline confirma señal y permite comparar representaciones.\n",
    "\n",
    "Evaluamos:\n",
    "- Ridge (std)\n",
    "- Ridge (PCA)\n",
    "- RandomForest (safe)\n",
    "- Ridge (solo controlables) — **enfoque de optimización**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb8335f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "def eval_reg(model, Xtr, Xte, ytr, yte, name):\n",
    "    model.fit(Xtr, ytr)\n",
    "    pred = model.predict(Xte)\n",
    "    mae = mean_absolute_error(yte, pred)\n",
    "    r2 = r2_score(yte, pred)\n",
    "    print(f\"{name:>24} | MAE={mae:.4f} | R2={r2:.4f}\")\n",
    "    return mae, r2\n",
    "\n",
    "ridge = Ridge(alpha=1.0, random_state=RANDOM_SEED)\n",
    "eval_reg(ridge, X_train_std, X_test_std, y_train, y_test, \"Ridge (std)\")\n",
    "eval_reg(ridge, Xtr_pca, Xte_pca, y_train, y_test, \"Ridge (PCA)\")\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators=400, random_state=RANDOM_SEED, n_jobs=-1)\n",
    "eval_reg(rf, X_train_safe, X_test_safe, y_train, y_test, \"RF (safe)\")\n",
    "\n",
    "# Solo controlables (más interpretable para optimización)\n",
    "if X_control_train_std.shape[1] >= 1:\n",
    "    eval_reg(ridge, X_control_train_std, X_control_test_std, y_train, y_test, \"Ridge (controlables)\")\n",
    "else:\n",
    "    print(\"No hay suficientes variables controlables disponibles para entrenar Ridge(controlables).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36021f09",
   "metadata": {},
   "source": [
    "## 13) Conclusiones — CRISP-ML (Data Preparation)\n",
    "\n",
    "1) Se estandarizaron timestamps y se integró clima por hora (enrichment).  \n",
    "2) Se generaron features de eficiencia/dinámica (kg/min, ratios) y temporales cíclicas.  \n",
    "3) Se aplicó binning para insights operativos.  \n",
    "4) Se codificaron categóricas con one-hot y frequency encoding.  \n",
    "5) Se implementó un preprocesamiento robusto (imputación + eliminación de columnas vacías/constantes + escalamiento).  \n",
    "6) Se aplicaron filtros (varianza, correlación, chi2, ANOVA) y extracción (PCA, FA).  \n",
    "\n",
    "**Hallazgo de relevancia:** variables de calendario como `is_weekend` suelen resultar **insignificantes** en este dominio (operación estandarizada). Por ello se excluyó `is_weekend` del set de entrenamiento final para reducir ruido y dimensionalidad, manteniendo representaciones cíclicas (`hour_sin/cos`) más informativas.\n",
    "\n",
    "**Siguiente paso (Modeling/Optimization):**\n",
    "- Interpretabilidad enfocada en **variables controlables** (palancas) y búsqueda de “sweet spots” (p.ej., duración óptima) con restricciones operativas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d0747d",
   "metadata": {},
   "source": [
    "## 14) Export (opcional)\n",
    "\n",
    "Exporta el dataset engineered para usar en la siguiente fase.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82819c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "engineered = X.copy()\n",
    "engineered[target_col] = y\n",
    "\n",
    "OUT = Path(\"/mnt/data/engineered_produccion_avance2.csv\")\n",
    "engineered.to_csv(OUT, index=False)\n",
    "print(\"Exportado:\", OUT, \"| shape:\", engineered.shape)\n",
    "engineered.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0969568b",
   "metadata": {},
   "source": [
    "---\n",
    "### Checklist antes de entregar\n",
    "1. Kernel → Restart & Run All  \n",
    "2. Verifica que corre de principio a fin (secuencial).  \n",
    "3. Renombra a `Avance2.#Equipo.ipynb` con tu número real.  \n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
